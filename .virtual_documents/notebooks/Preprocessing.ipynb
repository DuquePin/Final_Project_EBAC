





# Manipulação e transformação de dados
import numpy as np
import pandas as pd

# Visualização de dados
import seaborn as sns
import matplotlib.pyplot as plt

# Calculos estatísticos
from scipy.stats import pointbiserialr, f_oneway, chi2_contingency
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

# Ferramentas de análise
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Outros
from collections import defaultdict

%matplotlib inline


# Listando variáveis de interesse
vars = [
    'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)',
    'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)',
    'Tuition fees up to date',
    'Scholarship holder',
    'Course',
    'Application mode',
    'Target'
]

# Extraindo dados
data = pd.read_csv('../data/raw/train.csv')

# Selecionando apenas variáveis de interesse
df = data[vars].copy()

# Mapeando Target para estudar apenas o caso de evasão de alunos
df['Target'] = df['Target'].map({'Graduate': 0, 'Enrolled': 0, 'Dropout': 1})

df.head(5)


print('Numero de linhas: {0} \nNúmero de colunas: {1}'.format(df_sub.shape[0], df_sub.shape[1]))
df_sub.Target.value_counts(normalize=True)


# Visualizando estrutura dos dados
df_sub.info()


# Separando variáveis por sua natureza
vars_continuas = vars[0:4]
vars_bin = [col for col in df_sub.columns if set(df_sub[col].unique()) <= {0, 1}]
vars_nominais = list(set(vars[4:]).difference(set(vars_bin)))

df_sub[vars_bin].head(5)





# Criando metadados
metadados = pd.DataFrame({'Tipo dos Dados': df_sub.dtypes, 
                          'Número de Missing': df_sub.isna().sum(),
                          'Número de Classes': [df_sub[col].nunique() for col in df_sub]})

metadados['Papel'] = 'Independente'
metadados.loc[metadados.index == 'Target', 'Papel'] = 'Dependente'

metadados['Tipo da Variável'] = ''
metadados.loc[metadados.index.isin(vars_continuas), 'Tipo da Variável'] = 'Contínua'
metadados.loc[metadados.index.isin(vars_bin), 'Tipo da Variável'] = 'Binária'
metadados.loc[metadados.index.isin(vars_nominais), 'Tipo da Variável'] = 'Nominal'

metadados


def criar_tabela_IV(variavel, resposta):
    tab = pd.crosstab(variavel, resposta, margins=True, margins_name='total')
    
    rótulo_evento = tab.columns[0]
    rótulo_nao_evento = tab.columns[1]
    
    tab['pct_evento'] = tab[rótulo_evento] / tab.loc['total', rótulo_evento]
    tab['pct_nao_evento'] = tab[rótulo_nao_evento] / tab.loc['total', rótulo_nao_evento]
    
    # Tratamento para evitar log de zero
    tab['woe'] = np.log((tab['pct_evento'] + 1e-10) / (tab['pct_nao_evento'] + 1e-10))
    tab['iv_parcial'] = (tab['pct_evento'] - tab['pct_nao_evento']) * tab['woe']
    
    return tab

def calcular_IV(tabela_iv):
    return tabela_iv['iv_parcial'].sum()

def classificar_IV(iv):
    if iv < 0.02:
        return 'Inútil'
    elif 0.02 <= iv < 0.1:
        return 'Fraca'
    elif 0.1 <= iv < 0.3:
        return 'Média'
    elif 0.3 <= iv < 0.5:
        return 'Forte'
    else:
        return 'Overfit'

# Iterar sobre variáveis independentes
for var in metadados[metadados.Papel == 'Independente'].index:
    if var in vars_continuas:
        tabela_iv = criar_tabela_IV(pd.qcut(df_sub[var], 5, duplicates='drop'), df_sub.Target)
    else:
        tabela_iv = criar_tabela_IV(df_sub[var], df_sub.Target)
    
    # Calcular IV
    iv = calcular_IV(tabela_iv)
    metadados.loc[var, 'IV'] = iv
    
    # Exibir a tabela de IV para análise
    print(f'\nTabela IV para variável: {var}')
    display(tabela_iv)  # Exibir DataFrame no Jupyter Notebook ou ambiente compatível
    
    print(f'Information Value (IV): {iv} - Classificação: {classificar_IV(iv)}')


# Classificação do IV
metadados['Classificação IV'] = metadados['IV'].apply(classificar_IV)

# Exibir metadados finais
metadados





for col in df_sub:
    if col in vars_continuas:
        pd.qcut(df_sub[col], 5, duplicates='drop').value_counts(normalize=True).plot.bar()
        plt.show()
    else:
        df_sub[col].value_counts(normalize=True).plot.bar()
        plt.show()


def categorizar_variavel(df, coluna):
    if coluna in vars_continuas:
        df['categoria_temp'] = pd.qcut(df[coluna], q=5, duplicates='drop')
    else:
        df['categoria_temp'] = df[coluna]
    return df

def verificar_desbalanceamento(df):
    categorias = df['categoria_temp'].value_counts(normalize=True)
    return categorias.min(), categorias.max(), categorias

def criar_amostra_balanceada(df, categorias):
    min_categoria = categorias.idxmin()
    n_amostra = df[df['categoria_temp'] == min_categoria].shape[0]
    df_amostra = df.groupby('categoria_temp', group_keys=False, observed=False).apply(lambda x: x.sample(n=n_amostra))
    return df_amostra

def calcular_pesos(df_amostra, df):
    tab_amostra = df_amostra['categoria_temp'].value_counts()
    tab_populacao = df['categoria_temp'].value_counts()
    df_amostra['w'] = df_amostra['categoria_temp'].map(lambda x: tab_populacao[x] / tab_amostra[x])
    return df_amostra

def balancear_variavel(df, coluna, target):
    df = categorizar_variavel(df, coluna)
    min_proporcao, max_proporcao, categorias = verificar_desbalanceamento(df)
    
    if min_proporcao < 0.1 or max_proporcao > 0.4:
        df_amostra = criar_amostra_balanceada(df, categorias)
        df_amostra = calcular_pesos(df_amostra, df)
        return df_amostra
    else:
        df['w'] = 1
        return df


for col in df_sub:
    df_am = balancear_variavel(df_sub, col, 'Target')
    tab_iv = criar_tabela_IV(df_am['categoria_temp'], df_am.Target)
    iv = calcular_IV(tab_iv)
    dif = metadados.loc[col, "IV"]
    print(f'Variável {col}:')
    print(f'Antigo IV: {metadados.loc[col, "IV"]} - Classificação: {metadados.loc[col, "Classificação IV"]}')
    print(f'Novo IV: {iv} - Classificação: {classificar_IV(iv)}')
    print('='*50)





# Função para calcular a Correlação de Phi para Variáveis Binárias
def phi_coefficient(x, y):
    contingency_table = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(contingency_table)
    n = contingency_table.sum().sum()
    return np.sqrt(chi2 / n)

# Função para calcular a Correlação entre Variáveis Categóricas
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion_matrix)
    n = confusion_matrix.sum().sum()
    r, k = confusion_matrix.shape
    return np.sqrt(chi2 / (n * (min(r, k) - 1)))

# Função para calcular a Correlação entre Variáveis Contínuas e Binárias
def calculate_pointbiserialr(df, var_con, var_bin):
    corr, _ = pointbiserialr(x=df[var_bin], y=df[var_con])
    return corr

# Função para calcular o p-value entre Variáveis Contínuas e Nominais
def calculate_anova(df, var_con, var_nom):
    _, p_value = f_oneway(*[df[df[var_nom] == category][var_con] for category in df[var_nom].unique()])
    return p_value

# Dicionário para armazenar todas as correlações de todas as variáveis entre si
corr_vars = defaultdict(dict)

# Correlações de Pearson das Variáveis Contínuas entre si
corr_con = df_sub[vars_continuas].corr()

# Mapeando dicionário com as correlações das variáveis contínuas
for col in corr_con:
    corr_vars[col] = corr_con[col].to_dict()

# Mapeando dicionário com as correlações das variáveis contínuas e binárias
for var_con in vars_continuas:
    for var_bin in vars_bin:
        corr_vars[var_con][var_bin] = calculate_pointbiserialr(df_sub, var_con, var_bin)

# Mapeando dicionário com o p_value das variáveis contínuas e nominais
for var_con in vars_continuas:
    for var_nom in vars_nominais:
        corr_vars[var_con][var_nom] = calculate_anova(df_sub, var_con, var_nom)

# Mapeando dicionário com as correlações de phi das variáveis binárias entre si
for var_bin1 in vars_bin:
    for var_bin2 in vars_bin:
        corr_vars[var_bin1][var_bin2] = phi_coefficient(df_sub[var_bin1], df_sub[var_bin2])

# Mapeando dicionário com as correlações das variáveis binárias e nominais
for var_bin in vars_bin:
    for var_nom in vars_nominais:
        corr_vars[var_bin][var_nom] = cramers_v(df_sub[var_bin], df_sub[var_nom])

# Mapeando dicionário com as correlações das variáveis nominais entre si
for var_nom1 in vars_nominais:
    for var_nom2 in vars_nominais:
        corr_vars[var_nom1][var_nom2] = cramers_v(df_sub[var_nom1], df_sub[var_nom2])

# Criando um novo dicionário para armazenar as correlações organizadas
complete_corr_vars = defaultdict(dict)

# Preenchendo o novo dicionário com todas as correlações
for key in corr_vars:
    for other_key in corr_vars:
        if other_key in corr_vars[key]:
            complete_corr_vars[key][other_key] = corr_vars[key][other_key]
        elif key in corr_vars[other_key]:
            complete_corr_vars[key][other_key] = corr_vars[other_key][key]
        else:
            complete_corr_vars[key][other_key] = 0.0

# Salvando o dicionário de correlação em DataFrame
correlations = pd.DataFrame(complete_corr_vars)
correlations


# Visualizando gráfico das correlações
sns.clustermap(correlations, center=0, cmap='Blues')
plt.tight_layout()
plt.show()


# Definindo função para classificar VIF
def classificar_VIF(vif):
    if vif <= 1:
        return 'Baixa'
    elif 1 < vif < 5:
        return 'Moderada'
    elif 10 > vif >= 5:
        return 'Alta'
    elif vif >= 10:
        return 'Severa'

# Definindo função para cacular VIF da varável e salvar num dataframe
def criar_tabela_VIF(X):
    vif_table = pd.DataFrame()
    vif_table['Variável'] = X.columns
    vif_table['VIF'] = [vif(X.values, i) for i in range(X.shape[1])]
    vif_table['Classificação'] = [classificar_VIF(vif) for vif in vif_table['VIF']]
    return vif_table


# Calcular Variance Inflation Factor com as Variáveis Pré-selecionadas
vif_table_1 = criar_tabela_VIF(X=df_sub.drop('Target', axis=1))
vif_table_1





# Objeto para padronizar as variáveis contínuas
scaler = StandardScaler()

# Objeto para combinação linear
pca_4comp = PCA(n_components=4)  

# Transformando dados padronizados
pc_4comp = pca_4comp.fit_transform(scaler.fit_transform(df_sub[vars_continuas]))

# Extraindo a razão da variância explicada por componente 
explained_variance_4comp = pca_4comp.explained_variance_ratio_
print('Razão de Variância Explicada por Cada Componente:', explained_variance_4comp)


# Visualizando a variância explicada acumulada de cada componente
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_variance_4comp)+1), np.cumsum(explained_variance_4comp), marker='o', linestyle='--')
plt.xlabel('Número de Componentes')
plt.ylabel('Variância Explicada Acumulada')
plt.title('Variância Explicada por Componentes')
plt.show()


# Criando um dataframe com as componentes principais
df_pca_1 = pd.DataFrame(pc_4comp, 
                        columns=[f'PC{i+1}_vc' for i in range(len(vars_continuas))])

# Substituindo as variáveis originais pelos quatro componentes principais da combinação linear
df_sub = pd.concat([df_sub.drop(vars_continuas, axis=1), df_pca_1], axis=1)
df_sub.head(5)


# Recalculando o VIF
vif_table_2 = criar_tabela_VIF(X=df_sub.drop('Target', axis=1))
vif_table_2





# Criando componentes principais
pca_2comp = PCA(n_components=2)  
pc_2comp = pca_2comp.fit_transform(scaler.fit_transform(df[['Debtor', 'Tuition fees up to date']]))

print('Razão de Variância Explicada:', pca_2comp.explained_variance_ratio_)


# Criando dataframe com componentes principais
df_pca_2 = pd.DataFrame(pc_2comp, 
                        columns=[f'PC{i+1}_vb' for i in range(2)]
                       )

# Substituindo variáveis originais pelos dois componentes principais da combinação linear
df_sub = pd.concat([df_sub.drop('Tuition fees up to date', axis=1), df_pca_2], axis=1)
df_sub.head(5)


# Criando terceira tabela de VIF
vif_table_3 = criar_tabela_VIF(X=df_sub.drop('Target', axis=1))
vif_table_3





df_sub.head(5)











# Mapa das classes em Appliation mode
aplicacoes = {
    1: "1ª fase – contingente geral",
    2: "Portaria nº 612/93",
    5: "1ª fase - contingente especial (Ilha dos Açores)",
    7: "Titulares de outros cursos superiores",
    10: "Portaria nº 854-B/99",
    15: "Estudante internacional (bacharelado)",
    16: "1ª fase – contingente especial (Ilha da Madeira)",
    17: "2ª fase – contingente geral",
    18: "3ª fase – contingente geral",
    26: "Portaria n.º 533-A/99, alínea b2) (Plano Diferente)",
    27: "Portaria nº 533-A/99, item b3 (Outra Instituição)",
    39: "Maiores de 23 anos",
    42: "Transferência",
    43: "Mudança de curso",
    44: "Titulares de diploma de especialização tecnológica",
    51: "Mudança de instituição/curso",
    53: "Titulares de diplomas de ciclo curto",
    57: "Mudança de instituição/curso (Internacional)"
}

# Mapeando variável course para facilitar leitura
df_sub['Application mode'] = df_sub['Application mode'].map(aplicacoes)

# Removendo valores não identificados
df_sub.dropna(subset='Application mode', inplace=True)

df_sub.head(5)


# Mapa das classes em Course
cursos = {
    33: "Tecnologias de Produção de Biocombustíveis",
    171: "Animação e Design Multimédia",
    8014: "Serviço Social (atendimento noturno)",
    9003: "Agronomia",
    9070: "Design de Comunicação",
    9085: "Enfermagem Veterinária",
    9119: "Engenharia Informática",
    9130: "Equinicultura",
    9147: "Gestão",
    9238: "Serviço Social",
    9254: "Turismo",
    9500: "Enfermagem",
    9556: "Higiene Oral",
    9670: "Gestão de Publicidade e Marketing",
    9773: "Jornalismo e Comunicação",
    9853: "Ensino Básico",
    9991: "Gestão (atendimento noturno)"
}

# Mapeando variável course para facilitar leitura
df_sub['Course'] = df_sub['Course'].map(cursos)

# Removendo valores não identificados
df_sub.dropna(subset='Course', inplace=True)

df_sub.head(5)





# Aplicando Target Encoding
from sklearn.model_selection import KFold

# Copiar os dados originais
df_encoded = df_sub.copy()

kf = KFold(n_splits=5, shuffle=True, random_state=412)

# Para cada variável nominal
for var in vars_nominais:
    df_encoded[var + '_encoded'] = 0.0 
    # Para cada divisão de treino/validação
    for train_idx, val_idx in kf.split(df_encoded):
        # Calcular a média do target para cada categoria
        mean_target = df_encoded.iloc[train_idx].groupby(var)['Target'].mean()
        
        # Mapear a média no conjunto de validação
        df_encoded.loc[df_encoded.index[val_idx], var + '_encoded'] = df_encoded.loc[df_encoded.index[val_idx], var].map(mean_target)
    
    # Garantir que a coluna codificada seja do tipo float64
    df_encoded[var + '_encoded'] = df_encoded[var + '_encoded'].astype(float)

# Remover as colunas originais
df_encoded.drop(vars_nominais, inplace=True, axis=1)

df_encoded


df_encoded.dropna(inplace=True)


vif_table_4 = create_vif_table(X=df_encoded.drop('Target', axis=1))
vif_table_4





from scipy.stats import zscore

outliers_z = (np.abs(zscore(df_encoded)) > 3).sum(axis=0)
print('Outliers detectados:')
print(outliers_z)


# Plotando os outliers detectados
plt.figure(figsize=(10, 6))
plt.bar(outliers_z.index, outliers_z.values, color='lightblue')
plt.xlabel('Variáveis')
plt.ylabel('Número de Outliers')
plt.title('Número de Outliers Detectados por Z-Score (|Z| > 3)')
plt.xticks(rotation=90, ha='right')  
plt.tight_layout() 

plt.show()


X = df_encoded.drop('Target', axis=1)
X


import statsmodels.api as sm

# Definir as variáveis independentes (X) e dependente (y)
X = df_encoded.drop('Target', axis=1)
X = sm.add_constant(X)  
y = df_encoded['Target']

# Ajustar o modelo de regressão logística
model = sm.Logit(y, X)
result = model.fit()

# Obter influência dos pontos
influence = result.get_influence()

# Métricas de influência para regressão logística
cooks_d = influence.cooks_distance[0] 
dfbetas = influence.dfbetas  
leverage = influence.hat_matrix_diag  
deviance_residuals = result.resid_dev 

# Configuração do gráfico
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Cook's Distance
axes[0, 0].scatter(range(len(cooks_d)), cooks_d, color='red', alpha=0.5)
axes[0, 0].hlines(y=4/len(cooks_d), xmin=0, xmax=len(cooks_d), colors='blue', linestyle='--', label='Limite de Cook\'s Distance')
axes[0, 0].set_xlabel('Índice das Observações')
axes[0, 0].set_ylabel('Cook\'s Distance')
axes[0, 0].set_title('Cook\'s Distance - Influência das Observações')
axes[0, 0].legend()

# DFBETAS (Pegando o maior impacto por observação)
axes[0, 1].scatter(range(len(dfbetas)), np.max(np.abs(dfbetas), axis=1), color='red', alpha=0.5)
axes[0, 1].hlines(y=2/np.sqrt(len(dfbetas)), xmin=0, xmax=len(dfbetas), colors='blue', linestyle='--', label='Limite DFBETAS')
axes[0, 1].set_xlabel('Índice das Observações')
axes[0, 1].set_ylabel('DFBETAS')
axes[0, 1].set_title('DFBETAS - Influência nos Coeficientes')
axes[0, 1].legend()

# Leverage (hii)
axes[1, 0].scatter(range(len(leverage)), leverage, color='red', alpha=0.5)
axes[1, 0].hlines(y=2 * (X.shape[1] / len(leverage)), xmin=0, xmax=len(leverage), colors='blue', linestyle='--', label='Limite de Leverage')
axes[1, 0].set_xlabel('Índice das Observações')
axes[1, 0].set_ylabel('Leverage')
axes[1, 0].set_title('Leverage - Influência das Observações')
axes[1, 0].legend()

# Deviance Residuals 
axes[1, 1].scatter(range(len(deviance_residuals)), deviance_residuals, color='red', alpha=0.5)
axes[1, 1].hlines(y=[-2, 2], xmin=0, xmax=len(deviance_residuals), colors='blue', linestyle='--', label='Limite de Resíduos')
axes[1, 1].set_xlabel('Índice das Observações')
axes[1, 1].set_ylabel('Resíduos de Deviance')
axes[1, 1].set_title('Resíduos de Deviance - Ajuste do Modelo')
axes[1, 1].legend()

plt.tight_layout()
plt.show()


# Criar a figura com 2x2 subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Cook's Distance - Mapa de Densidade
hb1 = axes[0, 0].hexbin(range(len(cooks_d)), cooks_d, gridsize=50, cmap='Reds', mincnt=1)
axes[0, 0].hlines(y=4/len(cooks_d), xmin=0, xmax=len(cooks_d), colors='blue', linestyle='--', label='Limite de Cook\'s Distance')
axes[0, 0].set_title('Cook\'s Distance - Mapa de Densidade')
axes[0, 0].legend()
fig.colorbar(hb1, ax=axes[0, 0], label='Densidade')

# DFBETAS - Mapa de Densidade (considerando o maior impacto por observação)
dfbetas_max = np.max(np.abs(dfbetas), axis=1)
hb2 = axes[0, 1].hexbin(range(len(dfbetas_max)), dfbetas_max, gridsize=50, cmap='Reds', mincnt=1)
axes[0, 1].hlines(y=2/np.sqrt(len(dfbetas_max)), xmin=0, xmax=len(dfbetas_max), colors='blue', linestyle='--', label='Limite DFBETAS')
axes[0, 1].set_title('DFBETAS - Mapa de Densidade')
axes[0, 1].legend()
fig.colorbar(hb2, ax=axes[0, 1], label='Densidade')

# Leverage (hii) - Mapa de Densidade
hb3 = axes[1, 0].hexbin(range(len(leverage)), leverage, gridsize=50, cmap='Reds', mincnt=1)
axes[1, 0].hlines(y=2 * (X.shape[1] / len(leverage)), xmin=0, xmax=len(leverage), colors='blue', linestyle='--', label='Limite de Leverage')
axes[1, 0].set_title('Leverage (hii) - Mapa de Densidade')
axes[1, 0].legend()
fig.colorbar(hb3, ax=axes[1, 0], label='Densidade')

# Resíduos de Deviance - Mapa de Densidade
hb4 = axes[1, 1].hexbin(range(len(deviance_residuals)), deviance_residuals, gridsize=50, cmap='Reds', mincnt=1)
axes[1, 1].hlines(y=[-2, 2], xmin=0, xmax=len(deviance_residuals), colors='blue', linestyle='--', label='Limite de Resíduos')
axes[1, 1].set_title('Resíduos de Deviance - Mapa de Densidade')
axes[1, 1].legend()
fig.colorbar(hb4, ax=axes[1, 1], label='Densidade')

plt.tight_layout()
plt.show()





from sklearn.feature_selection import mutual_info_classif

info_gain = mutual_info_classif(X, y, random_state=412)

feature_importance = (pd.DataFrame({'Feature': X.columns, 'Importance': info_gain})
                       .sort_values(by='Importance', ascending=False))
feature_importance











from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = df_sub.copy()
df_scaled[df_sub.select_dtypes(include=['number']).columns] = scaler.fit_transform(df_sub[df_sub.select_dtypes(include=['number']).columns])

df_scaled



