











import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA

from ydata_profiling import ProfileReport





dados = pd.read_csv('../data/raw/train.csv')
dados.head(5)











# Mapeia variáveis com suas respectivas classes não identificadas
vars = {"Application mode": [3, 4, 35, 9, 12], 
        "Course": [979, 39], 
        "Previous qualification": [17, 15], 
        "Mother's qualification": [8, 15, 28, 7], 
        "Father's qualification": [7, 15, 21, 23, 24], 
        "Mother's occupation": [11, 38, 127], 
        "Father's occupation": [96, 39, 11, 12, 13, 19, 148, 22]}

# Mapeia variáveis com suas respectivas contagem de classes
contador_classes_por_var = {col: dados[col].value_counts() for col in vars.keys()}

# Visualiza a frequência das classes não identificadas de cada variável selecionada
for var, indices in vars.items():
    print(f'Para a variável {var}')
    contador_classes = contador_classes_por_var[var]
    total = contador_classes.sum()
    classes_selecionadas = contador_classes[contador_classes.index.isin(indices)]
    for index, contagem in enumerate(classes_selecionadas):
        print(indices[index], end=' - ')
        print(f'{(contagem / total) * 100:.3f} %')
    print()








df = dados

for k, v in vars.items():
    df = df[~df[k].isin(v)]

print('Nº de linhas antes: {}'.format(dados.shape[0]))
print('Nº de linhas depois: {}'.format(df.shape[0]))








print(f'Base de dados com:\n{df.shape[0]} linhas')
print(f'{df.shape[1]} colunas')


# Cria um dataframe para verificar se existe items nulos nas colunas
pd.DataFrame(index=[col for col in df], data=[df[col].isna().unique() for col in df])[0].unique()


# Verifica os tipos dos dados
df.dtypes.unique()


# Verifica se existe linhas duplicas
df.duplicated().unique()





# Lista com todas as variáveis quantitativas
quant = ['Previous qualification (grade)', 
       'Admission grade', 
       'Age at enrollment', 
       'Curricular units 1st sem (credited)',
       'Curricular units 1st sem (enrolled)',
       'Curricular units 1st sem (evaluations)',
       'Curricular units 1st sem (approved)',
       'Curricular units 1st sem (grade)',
       'Curricular units 1st sem (without evaluations)',
       'Curricular units 2nd sem (credited)',
       'Curricular units 2nd sem (enrolled)',
       'Curricular units 2nd sem (evaluations)',
       'Curricular units 2nd sem (approved)',
       'Curricular units 2nd sem (grade)',
       'Curricular units 2nd sem (without evaluations)',
       'Unemployment rate',
       'Inflation rate',
       'GDP']

# Lista com todas as variáveis qualitativas
qual = list(set(df.drop('id', axis=1).columns).difference(set(quant)))

# Visualizando apenas variáveis quantitativas
df.loc[:, quant]


# Visualizando apenas variáveis qualitativas
df.loc[:, qual]


# Criando os relatórios
profile1 = ProfileReport(df.loc[:, quant], title='Relatório de Análise Exploratória das Variáveis Quantitativas', 
                         explorative=True)
profile2 = ProfileReport(df.loc[:, qual], title='Relatório de Análise Exploratória das Variáveis Qualitativas', 
                         explorative=True)

# Salvando os relatórios na pasta de relatórios
profile1.to_file('../reports/Variáveis_Quantitativas.html')
profile2.to_file('../reports/Variáveis_Qualitativas.html')








sns.clustermap(df.loc[:, quant].corr(), cmap='Blues', center=0)
plt.tight_layout()
plt.show()





df.loc[:, quant].corr().apply(abs)[df.loc[:, quant].corr() != 1.0].max().sort_values(ascending=False).iloc[:5]





corr_quant_target = pd.concat([df.loc[:, quant], df['Target'].map({'Graduate': 0, 'Dropout': 1, 'Enrolled': 2})], axis=1).corr().apply(abs)['Target'].sort_values(ascending=False)
corr_quant_target


corr_quant_target.mean()





sns.clustermap(df.loc[:, qual].drop('Target', axis=1).corr(), cmap='Blues', center=0)
plt.tight_layout()
plt.show()





df.loc[:, qual].drop('Target', axis=1).corr().apply(abs)[df.loc[:, qual].drop('Target', axis=1).corr() != 1.0].max().sort_values(ascending=False).iloc[:5]





corr_qual_target = pd.concat([df.loc[:, qual].drop('Target', axis=1), df['Target'].map({'Graduate': 0, 'Dropout': 1, 'Enrolled': 2})], axis=1).corr().apply(abs)['Target'].sort_values(ascending=False)
corr_qual_target


corr_qual_target.mean()








demographs = df[["Marital status", "Nacionality", "Gender",
                "Age at enrollment", "Mother's qualification",
                "Father's qualification", "Mother's occupation",
                "Father's occupation"]].copy()

before_enrollment = df[["Application mode", "Application order",
                        "Previous qualification", "Previous qualification (grade)",
                        "Admission grade"]].copy()

socioeconomics = df[["Displaced", "Educational special needs",
                    "Debtor", "Tuition fees up to date",
                    "Scholarship holder", "Unemployment rate",
                    "Inflation rate", "GDP"]].copy()

academic_period = df[["Course", "Daytime/evening attendance", 
                      "Curricular units 1st sem (credited)", "Curricular units 1st sem (enrolled)",
                       "Curricular units 1st sem (evaluations)", "Curricular units 1st sem (approved)",
                       "Curricular units 1st sem (grade)", "Curricular units 1st sem (without evaluations)",
                       "Curricular units 2nd sem (credited)", "Curricular units 2nd sem (enrolled)", 
                       "Curricular units 2nd sem (evaluations)", "Curricular units 2nd sem (approved)", 
                       "Curricular units 2nd sem (grade)", "Curricular units 2nd sem (without evaluations)"]].copy()

groups = pd.Series([demographs, before_enrollment, socioeconomics, academic_period ], 
                   index=['demographs', 'before_enrollment', 'socioeconomics', 'academic_period'])


sns.clustermap(demographs.corr(), cmap='Blues', center=0, annot=True, fmt='.2f')
plt.show()





sns.clustermap(before_enrollment.corr(), cmap='Blues', center=0, annot=True, fmt='.2f')
plt.show()





sns.clustermap(socioeconomics.corr(), cmap='Blues', center=0, annot=True, fmt='.2f')
plt.show()





sns.clustermap(academic_period.corr(), cmap='Blues', center=0, annot=True, fmt='.2f')
plt.show()








# Encontrar correlações mais relevantes
corr_per_group = [] 
    
for group in groups:
    corr = group.corr().unstack().reset_index()
    corr.columns = ['Var A', 'Var B', 'Correlacao Linear']
    corr = (corr
            .query('`Var A` < `Var B`')
            .sort_values(by='Correlacao Linear', ascending=False)
            .reset_index(drop=True)
            .assign(Tipo = 'Positiva'))
    
    corr.loc[corr['Correlacao Linear'] < 0, 'Tipo'] = 'Negativa'
    corr['Correlacao Linear'] = abs(corr['Correlacao Linear'])
    
    corr_per_group.append(corr[corr['Correlacao Linear'] > 0.65])


# Variáveis Demográficas
corr_per_group[0]


# Variáveis Pré-matrícula
corr_per_group[1]


# Variáveis Socioeconômicas
corr_per_group[2]


# Variáveis Referent Desempenho Acadêmico no 1º Ano
corr_per_group[3]











# Transformar variável 'age at enrollment' de variável contínua para categórica
demographs['cat_Age at enrollment'] = pd.qcut(demographs['Age at enrollment'], q=4, labels=[1, 2, 3, 4])
demographs.drop('Age at enrollment', axis=1, inplace=True)
demographs.head(5)





# Transformar variáveis 'Previous qualification (grade)' e 'Admission grade' de variável contínua para categórica
before_enrollment['cat_Previous qualification (grade)'] = pd.qcut(before_enrollment['Previous qualification (grade)'], q=4, labels=[1, 2, 3, 4])
before_enrollment['cat_Admission grade'] = pd.qcut(before_enrollment['Admission grade'], q=4, labels=[1, 2, 3, 4])

before_enrollment.drop(['Previous qualification (grade)', 'Admission grade'], axis=1, inplace=True)

before_enrollment.head(5)





socioeconomics['cat_Unemployment rate'] = pd.qcut(socioeconomics['Unemployment rate'], q=4, labels=[1, 2, 3, 4])
socioeconomics['cat_Inflation rate'] = pd.qcut(socioeconomics['Inflation rate'], q=4, labels=[1, 2, 3, 4])
socioeconomics['cat_GDP'] = pd.qcut(socioeconomics['GDP'], q=4, labels=[1, 2, 3, 4])

socioeconomics.drop(['Unemployment rate', 'Inflation rate', 'GDP'], axis=1, inplace=True)

socioeconomics.head(5)





academic_period[f'cat_Curricular units 1st sem (grade)'] = pd.qcut(academic_period['Curricular units 1st sem (grade)'], q=4, labels=[1, 2, 3, 4])
academic_period[f'cat_Curricular units 2nd sem (grade)'] = pd.qcut(academic_period['Curricular units 2nd sem (grade)'], q=4, labels=[1, 2, 3, 4])

academic_period.drop(['Curricular units 1st sem (grade)', 'Curricular units 2nd sem (grade)'], axis=1, inplace=True)

academic_period.head(5)





from prince import MCA 

# Usando one-hot encoding para preparar os dados
demographs_encoded = pd.get_dummies(demographs, dtype='int')

# Criando objeto MCA
dem_mca = MCA(n_components=1, random_state=412, one_hot=False)

# Transformando a base de dados
dem_mca.fit_transform(demographs_encoded)

# Visualizando a variabilidade da componente
dem_mca.eigenvalues_summary['% of variance']


# Usando one-hot encoding para preparar os dados
benroll_encoded = pd.get_dummies(before_enrollment, dtype='int')

# Criando objeto MCA
ber_mca = MCA(n_components=1, random_state=412, one_hot=False)

# Transformando a base de dado
ber_mca.fit_transform(benroll_encoded)

# Visualizando a variabilidade da componente
ber_mca.eigenvalues_summary['% of variance']


# Usando one-hot encoding para preparar os dados
socioeconomics_encoded = pd.get_dummies(socioeconomics, dtype='int')

# Criando objeto MCA
sce_mca = MCA(n_components=1, random_state=412, one_hot=False)

# Transformando a base de dado
sce_mca.fit_transform(socioeconomics_encoded)

# Visualizando a variabilidade da componente
sce_mca.eigenvalues_summary['% of variance']


# Usando one-hot encoding para preparar os dados
academic_period_encoded = pd.get_dummies(academic_period, dtype='int')

# Criando objeto MCA
acp_mca = MCA(n_components=1, random_state=412, one_hot=False)

# Transformando a base de dado
acp_mca.fit_transform(academic_period_encoded)

# Visualizando a variabilidade da componente
acp_mca.eigenvalues_summary['% of variance']





d = pd.DataFrame()
d['Demographs'] = dem_mca.row_coordinates(demographs_encoded)[0]
d['Before Enrollment'] = ber_mca.row_coordinates(benroll_encoded)[0]
d['Socioeconomics'] = sce_mca.row_coordinates(socioeconomics_encoded)[0]
d['After Enrollment'] = acp_mca.row_coordinates(academic_period_encoded)[0]

correlation_matrix = d.corr()
correlation_matrix


sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation between Reduced Variables from Each Subset')
plt.show()








corr_inter_groups = []

for group in groups:
    for i in range(len(groups)):
        if group.columns[0] == groups.loc[groups.index[i]].columns[0]:
            continue
        else:
            corr_long = pd.concat([group, groups.loc[groups.index[i]]], axis=1).corr().stack().reset_index()
            corr_long.columns = ['Var1', 'Var2', 'Correlation']
            corr_long = corr_long[corr_long['Var1'] != corr_long['Var2']]
            corr_long = corr_long.drop_duplicates(subset=['Correlation'])
            corr_long = corr_long.reindex(corr_long['Correlation'].abs().sort_values(ascending=False).index)
            corr_inter_groups.append(corr_long)


for i in range(len(corr_inter_groups)):
    print('='*100)
    print(corr_inter_groups[i].iloc[:5])









