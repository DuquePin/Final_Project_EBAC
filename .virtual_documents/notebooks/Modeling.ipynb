





# Manipulação e transformação dos dados
import pandas as pd
import numpy as np

# Salvar modelos criado ajustados
from joblib import dump

# Modelagem
from sklearn.linear_model import LogisticRegression

# GridSearch
from sklearn.model_selection import train_test_split, GridSearchCV





# Extraindo dados para realizar a modelagem
df_desen_t = pd.read_feather('../data/processed/data_t.ftr')
df_desen_s = pd.read_feather('../data/processed/data_s.ftr')

# Visualizando estrutura das bases
display(df_desen_t.shape)
display(df_desen_s.shape)


# Separando em features (X) e target (y)

# Base de desenvolvimento com todas variáveis transformadas (Base T)
X_t = df_desen_t.drop(columns=['Target'])
y_t = df_desen_t.Target

# Base de desenvolvimento com subconjunto de variáveis selecionadas (Base S)
X_s = df_desen_s.drop(columns=['Target'])
y_s = df_desen_s.Target

# Visualizando estrutura das bases
display(X_s.shape)
display(X_t.shape)





# Separando base de desenvolvimento T em treino e teste
X_t_train, X_t_test, y_t_train, y_t_test = train_test_split(X_t, y_t, test_size=.2, random_state=412)

# Visualizando dados
display(X_t_train.head(5))
display(X_t_test.head(5))





# Criando modelo para realizar GridSearch
log_reg = LogisticRegression(max_iter=5000, random_state=412)

# Definindo parâmetros a testar
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100], 
    'penalty': ['l1', 'l2'], 
    'solver': ['liblinear', 'saga'] 
}

# Realizando GridSearch com dados de treino da base T
grid_search_T = GridSearchCV(log_reg, param_grid, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], refit=False, verbose=3, n_jobs=-1)
grid_search_T.fit(X_t_train, y_t_train)


# Coletando resultados do GridSearch
results_T = pd.DataFrame(grid_search_T.cv_results_)

# Calculando a média dos ranques
results_T['average_rank'] = results_T[['rank_test_accuracy', 'rank_test_precision', 'rank_test_recall', 'rank_test_f1', 'rank_test_roc_auc']].mean(axis=1)

# Visualizando
results_T.head(5)





# Função para destacar valores
def highlight_values(s):
    if s.name == 'params': 
        return [''] * len(s)
    if s.name in ['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time']:  
        is_min = s == s.min()
        return ['background-color: blue' if v else '' for v in is_min]
    if s.name.startswith(('mean_test', 'std_test')):  
        is_max = s == s.max()
        return ['background-color: blue' if v else '' for v in is_max]
    if s.name.startswith('rank_test'):  
        is_min = s == s.min()
        return ['background-color: blue' if v else '' for v in is_min]
    if s.name == 'average_rank':
        is_min = s == s.min()
        return ['background-color: blue' if v else '' for v in is_min]
    return [''] * len(s)  


# Criando visualização dos parâmetros com destaque
selected_columns = ['params', 'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time']
for col in results_T:
    if col.startswith('mean_test') or col.startswith('std_test') or col.startswith('rank_test') or col == 'average_rank':
        selected_columns.append(col)

# Criando objeto Style
results_highlighted_T = results_T[selected_columns].style.apply(highlight_values, subset=selected_columns[1:], axis=0)

# Visualizando
results_highlighted_T





# Melhores parâmetros testados
results_T.iloc[17]['params']





# Criando um modelo com os melhores parâmetros encontrados
logistic_t = LogisticRegression(max_iter=5000, random_state=412, C=100, penalty='l1', solver='saga')

# Ajustando modelo aos dados de treino da base T
logistic_t.fit(X_t_train, y_t_train)

# Salvando o modelo para avaliação no próximo notebook
dump(logistic_t, '../src/models/logistic_regression_model_base_T.joblib')
print('Modelo salvo com sucesso!')





# Separando base de desenvolvimento S em treino e teste
X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y_s, test_size=.2, random_state=412)

# Visualizando dados
display(X_s_train.head(5))
display(X_s_test.head(5))





# Realizando GridSearch com dados de treino da base S
grid_search_S = GridSearchCV(log_reg, param_grid, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], refit=False, verbose=3, n_jobs=-1)
grid_search_S.fit(X_s_train, y_s_train)


# Coletando resultados do GridSearch
results_S = pd.DataFrame(grid_search_S.cv_results_)

# Calculando a média dos ranques
results_S['average_rank'] = results_S[['rank_test_accuracy', 'rank_test_precision', 'rank_test_recall', 'rank_test_f1', 'rank_test_roc_auc']].mean(axis=1)

# Visualizando resultados
results_S.head(5)





# Criando visualização dos parâmetros com destaque
selected_columns = ['params', 'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time']
for col in results_S:
    if col.startswith('mean_test') or col.startswith('std_test') or col.startswith('rank_test') or col == 'average_rank':
        selected_columns.append(col)

# Criando objeto Style
results_highlighted_S = results_S[selected_columns].style.apply(highlight_values, subset=selected_columns[1:], axis=0)

# Visualizando
results_highlighted_S





# Melhores parâmetros testados
results_S.iloc[13]['params']





# Criando um modelo com os melhores parâmetros encontrados
logistic_s = LogisticRegression(max_iter=5000, random_state=412, C=10, penalty='l1', solver='saga')

# Ajustando modelo aos dados de treino da base S
logistic_s.fit(X_s_train, y_s_train)

# Salvando o modelo para avaliação no próximo notebook
dump(logistic_s, '../src/models/logistic_regression_model_base_S.joblib')
print('Modelo salvo com sucesso!')



